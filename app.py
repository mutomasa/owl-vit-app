"""
OWL-ViT Streamlit Application
å˜ä¸€è²¬ä»»åŸå‰‡ã«å¾“ã„ã€å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ©Ÿèƒ½ã‚’çµ±åˆã—ãŸãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

import streamlit as st
import torch
from PIL import Image
from typing import Optional, List

# ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from image_processor import ImageVisualizer
from model_manager import (
    ModelLoader, TextQueryProcessor, DetectionProcessor, 
    ImageGuidedDetectionProcessor
)
from ui_components import SidebarManager, InputManager, ResultsManager
from visualization import FlowVisualizationManager


class OWLViTApp:
    """OWL-ViTã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–"""
        self.setup_page_config()
        self.model = None
        self.processor = None
    
    def setup_page_config(self) -> None:
        """ãƒšãƒ¼ã‚¸è¨­å®šã‚’è¡Œã„ã¾ã™"""
        st.set_page_config(
            page_title="OWL-ViT ç‰©ä½“æ¤œå‡º",
            page_icon="ğŸ¦‰",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
        st.markdown("""
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;500;700&display=swap');
        
        .main .block-container {
            font-family: 'Noto Sans JP', sans-serif;
        }
        
        .stButton > button {
            font-family: 'Noto Sans JP', sans-serif;
        }
        
        .stSelectbox > div > div > div {
            font-family: 'Noto Sans JP', sans-serif;
        }
        
        .stTextInput > div > div > div > input {
            font-family: 'Noto Sans JP', sans-serif;
        }
        </style>
        """, unsafe_allow_html=True)
    
    def display_header(self) -> None:
        """ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¡¨ç¤ºã—ã¾ã™"""
        st.title("ğŸ¦‰ OWL-ViT Object Detection")
        st.markdown("""
        ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€OWL-ViTï¼ˆVision Transformer for Open-World Localizationï¼‰ã‚’ä½¿ç”¨ã—ã¦
        ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã¾ãŸã¯ç”»åƒã‚¯ã‚¨ãƒªã«ã‚ˆã‚‹ç‰©ä½“æ¤œå‡ºã‚’è¡Œã„ã¾ã™ã€‚
        
        **ç‰¹å¾´:**
        - ğŸ¯ **ã‚·ãƒ³ãƒ—ãƒ«æ¤œå‡º**: åŸºæœ¬çš„ãªç‰©ä½“æ¤œå‡º
        - ğŸ”¤ **ãƒ†ã‚­ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰æ¤œå‡º**: ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã§ç‰©ä½“ã‚’æ¤œå‡º
        - ğŸ–¼ï¸ **ç”»åƒã‚¬ã‚¤ãƒ‰æ¤œå‡º**: é¡ä¼¼ç”»åƒã§ç‰©ä½“ã‚’æ¤œå‡º
        - ğŸ“· **å¤šæ§˜ãªç”»åƒå…¥åŠ›**: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€URLã€ã‚«ãƒ¡ãƒ©æ’®å½±å¯¾å¿œ
        - ğŸ” **ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢**: æ¤œç´¢æ©Ÿèƒ½ã¨ã‚¯ã‚¨ãƒªææ¡ˆ
        - ğŸ”§ **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªè¨­å®š**
        - ğŸ“Š **è©³ç´°ãªæ¤œå‡ºçµæœã®è¡¨ç¤º**
        """)
    
    def initialize_model(self, model_name: str) -> bool:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™
        
        Args:
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
            
        Returns:
            åˆæœŸåŒ–ãŒæˆåŠŸã—ãŸå ´åˆTrue
        """
        if self.model is None or self.processor is None:
            self.model, self.processor = ModelLoader.load_model_and_processor(model_name)
            
        if self.model is None or self.processor is None:
            st.error("ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        return True
    
    def run_text_guided_detection(
        self,
        image: Image.Image,
        text_queries: List[str],
        confidence_threshold: float,
        translation_method: str = "è¾æ›¸ç¿»è¨³ã®ã¿"
    ) -> Optional[Image.Image]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰æ¤œå‡ºã‚’å®Ÿè¡Œã—ã¾ã™
        
        Args:
            image: å…¥åŠ›ç”»åƒ
            text_queries: ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
            confidence_threshold: ä¿¡é ¼åº¦é–¾å€¤
            
        Returns:
            å¯è¦–åŒ–ã•ã‚ŒãŸçµæœç”»åƒã€å¤±æ•—æ™‚ã¯None
        """
        try:
            from translator import JapaneseTranslator
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
            st.subheader("ğŸ” æ¤œå‡ºãƒ—ãƒ­ã‚»ã‚¹")
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_queries = TextQueryProcessor.format_text_queries(text_queries, translation_method)
            
            # ä½¿ç”¨ã•ã‚Œã‚‹ã‚¯ã‚¨ãƒªã‚’è¡¨ç¤º
            st.write("**ä½¿ç”¨ã•ã‚Œã‚‹ã‚¯ã‚¨ãƒª:**")
            for i, query_list in enumerate(formatted_queries):
                st.write(f"ã‚¯ã‚¨ãƒª {i+1}: {', '.join(query_list)}")
            
            # å…¥åŠ›ã®æº–å‚™
            inputs = DetectionProcessor.prepare_inputs(
                self.processor, image, formatted_queries
            )
            if inputs is None:
                return None
            
            # æ¨è«–ã®å®Ÿè¡Œ
            outputs = DetectionProcessor.run_inference(self.model, inputs)
            if outputs is None:
                return None
            
            # çµæœã®å¾Œå‡¦ç†ï¼ˆã‚ˆã‚Šä½ã„ä¿¡é ¼åº¦é–¾å€¤ã§è©¦è¡Œï¼‰
            target_sizes = torch.tensor([image.size[::-1]])
            
            # ã‚ˆã‚Šä½ã„ä¿¡é ¼åº¦é–¾å€¤ã§è©¦è¡Œ
            thresholds_to_try = [
                confidence_threshold, 
                confidence_threshold * 0.7, 
                confidence_threshold * 0.5,
                confidence_threshold * 0.3,
                confidence_threshold * 0.1,  # éå¸¸ã«ä½ã„é–¾å€¤ã‚‚è©¦è¡Œ
                0.05,  # ã•ã‚‰ã«ä½ã„é–¾å€¤
                0.01   # æœ€ã‚‚ä½ã„é–¾å€¤
            ]
            results = None
            successful_threshold = None
            
            st.write("**ğŸ¯ ä¿¡é ¼åº¦é–¾å€¤ã®èª¿æ•´ã‚’è©¦è¡Œä¸­...**")
            
            for threshold in thresholds_to_try:
                st.write(f"- é–¾å€¤ {threshold:.3f} ã‚’è©¦è¡Œä¸­...")
                results = DetectionProcessor.post_process_results(
                    self.processor, outputs, target_sizes, threshold
                )
                if results and any(len(result["boxes"]) > 0 for result in results):
                    successful_threshold = threshold
                    st.success(f"âœ… ä¿¡é ¼åº¦é–¾å€¤ {threshold:.3f} ã§æ¤œå‡ºã«æˆåŠŸã—ã¾ã—ãŸ")
                    break
                else:
                    st.write(f"  âŒ é–¾å€¤ {threshold:.3f} ã§ã¯æ¤œå‡ºãªã—")
            
            if results is None or not any(len(result["boxes"]) > 0 for result in results):
                st.warning("âš ï¸ ã™ã¹ã¦ã®é–¾å€¤ã§æ¤œå‡ºçµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.write("**ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:**")
                st.write(f"- è©¦è¡Œã—ãŸé–¾å€¤: {thresholds_to_try}")
                st.write(f"- å…ƒã®ç”»åƒã‚µã‚¤ã‚º: {image.size}")
                st.write(f"- ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªæ•°: {len(text_queries)}")
                st.write("**ğŸ’¡ æ”¹å–„ã®ãƒ’ãƒ³ãƒˆ:**")
                st.write("- ä¿¡é ¼åº¦é–¾å€¤ã‚’ã•ã‚‰ã«ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„")
                st.write("- ã‚ˆã‚Šå…·ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã‚’è©¦ã—ã¦ãã ã•ã„")
                st.write("- ç”»åƒã®å“è³ªã‚„ç‰©ä½“ã®å¤§ãã•ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                return None
            
            # çµæœã®è¡¨ç¤º
            ResultsManager.display_detection_results(
                results, text_queries, confidence_threshold
            )
            
            # å¯è¦–åŒ–ç”»åƒã®ä½œæˆ
            if results and len(results) > 0:
                result = results[0]  # æœ€åˆã®çµæœã‚’ä½¿ç”¨
                boxes = result["boxes"].tolist()
                scores = result["scores"].tolist()
                
                # ãƒ©ãƒ™ãƒ«ã®å–å¾—ï¼ˆç¿»è¨³æƒ…å ±ã‚’å«ã‚€ï¼‰
                labels = []
                for label_idx in result["labels"].tolist():
                    if label_idx < len(text_queries):
                        original_query = text_queries[label_idx]
                        # ç¿»è¨³æƒ…å ±ã‚’è¿½åŠ 
                        if JapaneseTranslator.is_japanese(original_query):
                            english_translation = JapaneseTranslator.translate_japanese_to_english(original_query)
                            if english_translation != original_query:
                                labels.append(f"{original_query}({english_translation})")
                            else:
                                labels.append(original_query)
                        else:
                            labels.append(original_query)
                    else:
                        labels.append("unknown")
                
                # æ¤œå‡ºçµæœãŒã‚ã‚‹å ´åˆã®ã¿å¯è¦–åŒ–
                if len(boxes) > 0:
                    visualized_image = ImageVisualizer.create_detection_visualization(
                        image, boxes, scores, labels, confidence_threshold
                    )
                    return visualized_image
                else:
                    st.info("æ¤œå‡ºçµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    return None
            
            return None
            
        except Exception as e:
            st.error(f"ãƒ†ã‚­ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰æ¤œå‡ºã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            if st.session_state.get('debug_mode', True):
                st.write("**ğŸ” ã‚¨ãƒ©ãƒ¼ã®è©³ç´°æƒ…å ±:**")
                st.write(f"- ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
                st.write(f"- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}")
                st.write(f"- ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªæ•°: {len(text_queries)}")
                st.write(f"- ç”»åƒã‚µã‚¤ã‚º: {image.size}")
                st.write("**ğŸ’¡ è§£æ±ºç­–ã®ãƒ’ãƒ³ãƒˆ:**")
                st.write("- ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®æ•°ã‚’æ¸›ã‚‰ã—ã¦ã¿ã¦ãã ã•ã„")
                st.write("- ã‚ˆã‚Šç°¡å˜ãªã‚¯ã‚¨ãƒªã‚’è©¦ã—ã¦ãã ã•ã„")
                st.write("- ç”»åƒã®ã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦ã¿ã¦ãã ã•ã„")
            return None
    
    def run_image_guided_detection(
        self,
        image: Image.Image,
        query_image: Image.Image,
        confidence_threshold: float,
        nms_threshold: float
    ) -> Optional[Image.Image]:
        """
        ç”»åƒã‚¬ã‚¤ãƒ‰æ¤œå‡ºã‚’å®Ÿè¡Œã—ã¾ã™
        
        Args:
            image: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”»åƒ
            query_image: ã‚¯ã‚¨ãƒªç”»åƒ
            confidence_threshold: ä¿¡é ¼åº¦é–¾å€¤
            nms_threshold: NMSé–¾å€¤
            
        Returns:
            å¯è¦–åŒ–ã•ã‚ŒãŸçµæœç”»åƒã€å¤±æ•—æ™‚ã¯None
        """
        try:
            # å…¥åŠ›ã®æº–å‚™
            inputs = ImageGuidedDetectionProcessor.prepare_image_guided_inputs(
                self.processor, image, query_image
            )
            if inputs is None:
                return None
            
            # æ¨è«–ã®å®Ÿè¡Œ
            outputs = ImageGuidedDetectionProcessor.run_image_guided_inference(
                self.model, inputs
            )
            if outputs is None:
                return None
            
            # çµæœã®å¾Œå‡¦ç†
            target_sizes = torch.tensor([image.size[::-1]])
            results = ImageGuidedDetectionProcessor.post_process_image_guided_results(
                self.processor, outputs, target_sizes, confidence_threshold, nms_threshold
            )
            if results is None:
                return None
            
            # çµæœã®è¡¨ç¤º
            ResultsManager.display_image_guided_results(results, confidence_threshold)
            
            # å¯è¦–åŒ–ç”»åƒã®ä½œæˆ
            if results and len(results) > 0:
                result = results[0]  # æœ€åˆã®çµæœã‚’ä½¿ç”¨
                boxes = result["boxes"].tolist()
                scores = result["scores"].tolist()
                labels = ["similar object"] * len(boxes)  # ç”»åƒã‚¬ã‚¤ãƒ‰æ¤œå‡ºã§ã¯ãƒ©ãƒ™ãƒ«ã¯å›ºå®š
                
                visualized_image = ImageVisualizer.create_detection_visualization(
                    image, boxes, scores, labels, confidence_threshold
                )
                return visualized_image
            
            return None
            
        except Exception as e:
            st.error(f"ç”»åƒã‚¬ã‚¤ãƒ‰æ¤œå‡ºã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None
    
    def run(self) -> None:
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ«ãƒ¼ãƒ—"""
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã®è¡¨ç¤º
        self.display_header()
        
        # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
        selected_model = SidebarManager.create_model_selection()
        detection_mode = SidebarManager.create_detection_mode_selection()
        confidence_threshold, nms_threshold = SidebarManager.create_confidence_settings()
        visualization_type = SidebarManager.create_visualization_options()
        
        # å¯è¦–åŒ–ã®è¡¨ç¤º
        if st.session_state.get('show_flow_diagram', False):
            FlowVisualizationManager.display_flow_visualization()
            st.markdown("---")  # åŒºåˆ‡ã‚Šç·š
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        if not self.initialize_model(selected_model):
            st.stop()
        
        # å…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        image = InputManager.create_image_input_section()
        if image is None:
            st.info("ç”»åƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        # æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸå‡¦ç†
        if detection_mode == "ã‚·ãƒ³ãƒ—ãƒ«æ¤œå‡º":
            # ã‚·ãƒ³ãƒ—ãƒ«æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰
            st.subheader("ğŸ¯ ã‚·ãƒ³ãƒ—ãƒ«æ¤œå‡º")
            st.write("åŸºæœ¬çš„ãªç‰©ä½“æ¤œå‡ºã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
            
            # æ¤œå‡ºå®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸ” ã‚·ãƒ³ãƒ—ãƒ«æ¤œå‡ºã‚’å®Ÿè¡Œ", type="primary"):
                with st.spinner("ã‚·ãƒ³ãƒ—ãƒ«æ¤œå‡ºã‚’å®Ÿè¡Œä¸­..."):
                    # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ï¼ˆæ•°ã‚’åˆ¶é™ï¼‰
                    basic_queries = ["person", "car", "dog"]
                    visualized_image = self.run_text_guided_detection(
                        image, basic_queries, confidence_threshold, "è¾æ›¸ç¿»è¨³ã®ã¿"
                    )
                    
                    if visualized_image:
                        st.image(visualized_image, caption="æ¤œå‡ºçµæœ", use_container_width=True)
                        ResultsManager.create_download_section(visualized_image)
                    else:
                        st.warning("ã‚·ãƒ³ãƒ—ãƒ«æ¤œå‡ºã§ç‰©ä½“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        elif detection_mode == "ãƒ†ã‚­ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰æ¤œå‡º":
            result = InputManager.create_text_query_section()
            if result is None:
                st.info("ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                return
            
            text_queries, translation_method = result
            
            # æ¤œå‡ºå®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸ” ç‰©ä½“æ¤œå‡ºã‚’å®Ÿè¡Œ", type="primary"):
                with st.spinner("ç‰©ä½“æ¤œå‡ºã‚’å®Ÿè¡Œä¸­..."):
                    visualized_image = self.run_text_guided_detection(
                        image, text_queries, confidence_threshold, translation_method
                    )
                    
                    if visualized_image:
                        st.image(visualized_image, caption="æ¤œå‡ºçµæœ", use_container_width=True)
                        ResultsManager.create_download_section(visualized_image)
        
        elif detection_mode == "ç”»åƒã‚¬ã‚¤ãƒ‰æ¤œå‡º":
            query_image = InputManager.create_query_image_section()
            if query_image is None:
                st.info("ã‚¯ã‚¨ãƒªç”»åƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                return
            
            # æ¤œå‡ºå®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸ” é¡ä¼¼ç‰©ä½“æ¤œå‡ºã‚’å®Ÿè¡Œ", type="primary"):
                with st.spinner("é¡ä¼¼ç‰©ä½“æ¤œå‡ºã‚’å®Ÿè¡Œä¸­..."):
                    visualized_image = self.run_image_guided_detection(
                        image, query_image, confidence_threshold, nms_threshold
                    )
                    
                    if visualized_image:
                        st.image(visualized_image, caption="æ¤œå‡ºçµæœ", use_container_width=True)
                        ResultsManager.create_download_section(visualized_image)
        
        # æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        self.display_info_section()
    
    def display_info_section(self) -> None:
        """æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã™"""
        with st.expander("â„¹ï¸ OWL-ViTã«ã¤ã„ã¦"):
            st.markdown("""
            **OWL-ViTï¼ˆVision Transformer for Open-World Localizationï¼‰**ã¯ã€
            ã‚ªãƒ¼ãƒ—ãƒ³ãƒœã‚­ãƒ£ãƒ–ãƒ©ãƒªç‰©ä½“æ¤œå‡ºã®ãŸã‚ã®Vision Transformerãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
            
            **ä¸»ãªç‰¹å¾´:**
            - ğŸ¯ **ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¤œå‡º**: äº‹å‰ã«å­¦ç¿’ã—ãŸç‰©ä½“ã‚¯ãƒ©ã‚¹ä»¥å¤–ã‚‚æ¤œå‡ºå¯èƒ½
            - ğŸ”¤ **ãƒ†ã‚­ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰æ¤œå‡º**: ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã§ç‰©ä½“ã‚’æ¤œå‡º
            - ğŸ–¼ï¸ **ç”»åƒã‚¬ã‚¤ãƒ‰æ¤œå‡º**: é¡ä¼¼ç”»åƒã§ç‰©ä½“ã‚’æ¤œå‡º
            - ğŸ§  **CLIPãƒ™ãƒ¼ã‚¹**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªç†è§£èƒ½åŠ›
            
            **æŠ€è¡“è©³ç´°:**
            - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: Vision Transformer + CLIP
            - å…¥åŠ›: ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆ/ç”»åƒã‚¯ã‚¨ãƒª
            - å‡ºåŠ›: ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ + ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
            
            **å‚è€ƒè³‡æ–™:**
            - [OWL-ViT Paper](https://arxiv.org/abs/2205.06230)
            - [Hugging Face Documentation](https://huggingface.co/docs/transformers/model_doc/owlvit)
            """)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    app = OWLViTApp()
    app.run()


if __name__ == "__main__":
    main() 